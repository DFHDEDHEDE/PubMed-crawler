import requests
import json
from typing import Optional
import requests
from bs4 import BeautifulSoup
import re
import time
import pandas as pd
from urllib.parse import urljoin
from tqdm import tqdm
import logging
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_user_input():
    website = input('Enter your PubMed search URL: ')
    num_papers = int(input('Number of papers to retrieve: '))
    use_of_ai = input('Do you want to use AI to screen the results (T) or not (F): ')
    if use_of_ai == 'T':
        user_description = input('Enter your description: ')
    elif use_of_ai == 'F':
        user_description = 'NONE'
    else:
        user_description = 'FALSE'
        print("Invalid input. Please say 'T' or 'F'.")
        sys.exit(0)
    num_keywords = int(input('Number of keywords: '))

    keywords = []
    for i in range(num_keywords):
        keyword = input(f'Enter keyword {i + 1}: ')
        keywords.append(keyword)

    return website, num_papers, use_of_ai, user_description, keywords

def fetch_page(url, retries=3, delay=2):
    for attempt in range(retries):
        try:
            response = requests.get(url)
            response.raise_for_status()  
            return response.text
        except requests.RequestException as e:
            logger.error(f"Request error ({attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay * (2 ** attempt))  
    logger.error(f"Failed to fetch {url} after {retries} attempts")
    return None

def extract_paper_links(html_content):
    if not html_content:
        return []

    soup = BeautifulSoup(html_content, 'html.parser')
    links = []

    for link in soup.select('a.docsum-title'):
        href = link.get('href')
        if href:
            links.append(urljoin('https://pubmed.ncbi.nlm.nih.gov', href))

    return links

def extract_title(html_content):
    if not html_content:
        return None

    soup = BeautifulSoup(html_content, 'html.parser')

    title_element = soup.select_one('.heading-title')
    if not title_element:
        title_element = soup.select_one('h1.article-title')
    if not title_element:
        title_element = soup.select_one('h1')

    if title_element:
        return title_element.get_text(strip=True)
    else:
        return "Title not found"

def extract_abstract(html_content):
    if not html_content:
        return None

    soup = BeautifulSoup(html_content, 'html.parser')
    abstract_element = soup.select_one('.abstract-content')

    if not abstract_element:
        abstract_element = soup.select_one('.abstract')

    if abstract_element:
        return abstract_element.get_text(strip=True)
    else:
        return None

def llm_response(
    prompt: str,
    model: str = "deepseek-r1:14b",  
    base_url: str = "http://localhost:11434",  # Ollama application
    temperature: float = 0,
    max_tokens: Optional[int] = None,
    system_prompt: Optional[str] = None
) -> str:

    try:
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature
            }
        }
        
        if system_prompt:
            data["system"] = system_prompt
        
        if max_tokens:
            data["options"]["num_predict"] = max_tokens
        
        # send requirements to Ollama API
        response = requests.post(
            f"{base_url}/api/generate",
            headers={"Content-Type": "application/json"},
            data=json.dumps(data),
            timeout=60
        )
        
        response.raise_for_status()
        
        result = response.json()
        return result.get("response", "").strip()
        
    except requests.exceptions.ConnectionError:
        return "错误: 无法连接到 Ollama 应用，请确保 Ollama 桌面应用正在运行"
    except requests.exceptions.Timeout:
        return "错误: 请求超时，请检查模型是否已加载"
    except Exception as e:
        return f"错误: {str(e)}"

def analyze_text(text, keywords):
    if not text:
        return 0, 0, 0

    total_words = len(text.split())
    keyword_matches = 0

    for keyword in keywords:
        keyword_matches += text.lower().count(keyword.lower())

    frequency = keyword_matches / total_words if total_words > 0 else 0
    return total_words, keyword_matches, frequency

def get_paper_count_bs(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    meta_tag = soup.find('meta', {'name': 'log_resultcount'})
    
    if meta_tag:
        count = meta_tag.get('content')
        return int(count) if count else None
    else:
        return None

def main():
    base_url, num_papers, ai_or_not, description, keywords = get_user_input()
    papers_data = []
    page_number = 1
    papers_fetched = 0
    base_url_content = fetch_page(base_url)
    total_count = get_paper_count_bs(base_url_content)
    #print(total_count)

    with tqdm(total=total_count, desc="Fetching papers") as pbar:
        while papers_fetched < total_count:
            page_url = f"{base_url}&page={page_number}"
            logger.info(f"Fetching page {page_number}: {page_url}")

            page_content = fetch_page(page_url)
            if not page_content:
                break

            paper_links = extract_paper_links(page_content)
            if not paper_links:
                logger.info("No more papers found")
                break

            for link in paper_links:
                if papers_fetched >= total_count:
                    break

                paper_content = fetch_page(link)
                if not paper_content:
                    continue

                title = extract_title(paper_content)
                abstract = extract_abstract(paper_content)

                title_words, title_matches, title_freq = analyze_text(title, keywords)
                abstract_words, abstract_matches, abstract_freq = analyze_text(abstract, keywords)

                combined_freq = (title_freq * 0.4 + abstract_freq * 0.6) if (title_freq + abstract_freq) > 0 else 0

                pmid_match = re.search(r'/(\d+)/', link)
                pmid = pmid_match.group(1) if pmid_match else "Unknown"

                papers_data.append({
                    'PMID': pmid,
                    'URL': link,
                    'Title': title,
                    'Abstract': abstract[:200] + '...' if abstract else "Abstract not available",
                    'TitleWords': title_words,
                    'TitleMatches': title_matches,
                    'TitleFrequency': title_freq,
                    'AbstractWords': abstract_words,
                    'AbstractMatches': abstract_matches,
                    'AbstractFrequency': abstract_freq,
                    'CombinedFrequency': combined_freq
                })

                papers_fetched += 1
                pbar.update(1)

                time.sleep(0.5)

            page_number += 1
            if page_number > 100:
                logger.warning("Reached maximum page limit")
                break

    num_analyse = num_papers*3
    if papers_data:
        df = pd.DataFrame(papers_data)
        df = df.sort_values(by='CombinedFrequency', ascending=False).head(num_analyse).reset_index(drop=True)

        #print("\nTop papers by combined keyword frequency:")
        #print(df.head(num_analyse)[
                  #['PMID', 'Title', 'CombinedFrequency', 'TitleFrequency', 'AbstractFrequency', 'Abstract']])

        #df.to_csv('pubmed_results.csv', index=False)
        #print("\nFull results saved to pubmed_results.csv")
    else:
        print("No papers found matching the criteria.")
    
    if ai_or_not == 'T':
        prompt_description = "对文献的要求: " + description
        prompt_num = " 需要筛选出的文献数量: " + str(num_papers)
        prompt_paperlist = " 论文列表: " + str(df)
        requirement = """任务：文献筛选
        需求：根据对文献的要求，查看论文的标题与摘要，根据要求与论文的契合程度从0-100打分，根据打分结果筛选出要求数量的文献。论文列表附在后文。

        **请严格按此格式输出：**
        1. 论文编号、0-100打分、一句话理由
        2. 论文编号、0-100打分、一句话理由
        ...

        不要解释，不要思考过程，直接输出结果。"""
        promptlist = [prompt_description, prompt_num, requirement, prompt_paperlist]
        prompt = "".join(promptlist)
        response = llm_response(prompt)
        print(response)
    elif ai_or_not == 'F':
        print("\nTop papers by combined keyword frequency:")
        print(df.head(num_papers)[
                  ['PMID', 'Title', 'CombinedFrequency', 'TitleFrequency', 'AbstractFrequency', 'Abstract']])

        #df.to_csv('pubmed_results.csv', index=False)
        #print("\nFull results saved to pubmed_results.csv")
    else:
        print("Invalid input. Please say 'T' or 'F'.")
        sys.exit(0)

if __name__ == "__main__":
    main()
